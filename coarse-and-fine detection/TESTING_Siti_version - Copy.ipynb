{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\" #please put your GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from io_ import score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into train, test, val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install split-folders tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import splitfolders  # or import split_folders  #split into train, test, val\n",
    "\n",
    "# # Split with a ratio.\n",
    "# # To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "# splitfolders.ratio(data_dir, output=\"Splitteddata/\", seed=1337, ratio=(.8, .1, .1), group_prefix=None) # default values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'Splitteddata/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "img_height = 225\n",
    "img_width = 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  test_dir,\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  label_mode = 'int',                  #sparse categorical loss\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixels values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = normalized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in test_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score each of the images based on the likelihood it belongs to a class. Larger scores indicate higher likelihood.\n",
    "\n",
    "Classes are arranged alphabeticall, [1_Capitol, 1_Chijmes,..., 1_scis, americano, ... taco]. Screen-shot is attached.\n",
    "\n",
    "The first thirteen belong to buidings and the next fifty nine belong to food, this makes for a total of 72 classes.\n",
    "\n",
    "Scores should take the form of a [Nx72] array, where N is the number of test images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting number of images in test folder (required to create matrix later) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_images = len(np.concatenate([i for x, i in test_ds], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 3082  #change accordingly   #hard-coded  #run this if cell above takes too long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model reconstruction from JSON file\n",
    "with open('Siti(27).json', 'r') as json_file:\n",
    "    json_savedModel= json_file.read()\n",
    "\n",
    "test_model = tf.keras.models.model_from_json(json_savedModel)\n",
    "test_model.summary()\n",
    "\n",
    "optim = SGD(lr=0.01, momentum=0.9)\n",
    "\n",
    "# test_model.compile(loss='sparse_categorical_crossentropy',\n",
    "#               optimizer=optim,\n",
    "#               metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k = 1)])\n",
    "\n",
    "test_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optim,\n",
    "              metrics=['accuracy', tf.keras.metrics.SparseTopKCategoricalAccuracy(k = 5)])\n",
    "\n",
    "# Load weights into the new model\n",
    "test_model.load_weights('Siti(27).h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating matrix for ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = np.zeros([num_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for images, labels in test_ds:\n",
    "    \n",
    "    print(labels)\n",
    "    labels_array = labels.numpy()\n",
    "    print(labels_array)\n",
    "    for lbl in labels_array:\n",
    "        ground_truth[i] = lbl\n",
    "        i+=1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = ground_truth.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forming matrix for predicted scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_scores = np.zeros([num_images, 72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for images, labels in test_ds:\n",
    "    img_scores = test_model.predict(images)\n",
    "    print(img_scores)\n",
    "    for score in img_scores:\n",
    "        print(score)\n",
    "        predicted_scores[i] = score\n",
    "        i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(predicted_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(img_view[0].numpy().astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining accuracy (Comparing ground truth with predicted scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io_ import score\n",
    "print('overall accuracy')\n",
    "scores = predicted_scores\n",
    "gt = ground_truth \n",
    "top1, top5 = score(scores, gt, 5)\n",
    "print('percentage top1 accuracy:', top1)\n",
    "print('percentage top5 accuracy:', top5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating the evaluation functions on the first two test images\n",
    "\n",
    "You need not modify code beyond this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io_ import score\n",
    "print('overall accuracy')\n",
    "\n",
    "# simulated scores\n",
    "scores = np.zeros([2,72])\n",
    "scores[0,0] = 1000\n",
    "scores[1,3] = 1000\n",
    "scores[1,21] = 999\n",
    "\n",
    "# Ground-truth: the first image belongs to class 0; the second image to class 13.\n",
    "gt = np.array([0,21]) \n",
    "\n",
    "top1, top5 = score(scores, gt, 5)\n",
    "print('percentage top1 accuracy:', top1)\n",
    "print('percentage top5 accuracy:', top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy on building only')\n",
    "mask = gt<13\n",
    "top1, top5 = score(scores[mask,:13], gt[mask], 5)\n",
    "print('percentage top1 accuracy:', top1)\n",
    "print('percentage top5 accuracy:', top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy on food only')\n",
    "mask = gt>=13\n",
    "top1, top5 = score(scores[mask,13:], gt[mask]-13, 5)\n",
    "print('percentage top1 accuracy:', top1)\n",
    "print('percentage top5 accuracy:', top5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}